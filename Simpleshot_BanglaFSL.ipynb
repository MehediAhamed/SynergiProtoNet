{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWhTxhYS7TLf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    colab = True\n",
        "except:\n",
        "    colab = False"
      ],
      "metadata": {
        "id": "NnxSUVIK73xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if colab is True:\n",
        "    # Running in Google Colab\n",
        "    # Clone the repo\n",
        "    !git clone https://github.com/sicara/easy-few-shot-learning\n",
        "    %cd easy-few-shot-learning\n",
        "    !pip install .\n",
        "else:\n",
        "    # Run locally\n",
        "    # Ensure working directory is the project's root\n",
        "    # Make sure easyfsl is installed!\n",
        "    %cd .."
      ],
      "metadata": {
        "id": "c5vrGDXf7_Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/BanglaLekha_Isolated_FSL.zip -d /content/data"
      ],
      "metadata": {
        "id": "52Pw8AbY3aHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import random\n",
        "from statistics import mean\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import torchvision\n",
        "import torch.utils.data"
      ],
      "metadata": {
        "id": "hhiI49Vl8DkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 30\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "JE-g7hat8EWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Episodic Training"
      ],
      "metadata": {
        "id": "0z4qdEF-9Nhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_way = 5\n",
        "n_shot = 1\n",
        "n_query = 10\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "n_workers = 0"
      ],
      "metadata": {
        "id": "ZoaV-tbj8HJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class BengaliCharactersDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.classes = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
        "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "        self.samples = []\n",
        "\n",
        "        for class_name in self.classes:\n",
        "            class_dir = os.path.join(root_dir, class_name)\n",
        "            for img_file in os.listdir(class_dir):\n",
        "                if img_file.endswith('.png'):\n",
        "                    self.samples.append((os.path.join(class_dir, img_file), class_name))\n",
        "\n",
        "        # Debug: Print the first few samples\n",
        "        print(\"First few samples:\")\n",
        "        print(self.samples[:5])  # Adjust the number to print more or fewer samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label_str = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Convert label from string to integer\n",
        "        label = self.class_to_idx[label_str]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def get_labels(self):\n",
        "        return [label for _, label in self.samples]\n"
      ],
      "metadata": {
        "id": "Ii6KTrFt1u5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 84  # Adjusted for ResNet, which typically expects larger input sizes\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_set = BengaliCharactersDataset(root_dir='/content/data/BanglaLekha_Isolated_FSL/Train', transform=train_transforms)\n",
        "test_set = BengaliCharactersDataset(root_dir='/content/data/BanglaLekha_Isolated_FSL/Test', transform=test_transforms)\n"
      ],
      "metadata": {
        "id": "3lTdyVps2RLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from easyfsl.samplers import TaskSampler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "n_tasks_per_epoch = 500\n",
        "n_validation_tasks = 100\n",
        "\n",
        "# Directly use the test set for validation\n",
        "val_set = test_set\n",
        "\n",
        "train_set.get_labels = lambda: [\n",
        "    instance[1] for instance in train_set\n",
        "]\n",
        "\n",
        "val_set.get_labels = lambda: [\n",
        "    instance[1] for instance in val_set\n",
        "]\n",
        "\n",
        "# Those are special batch samplers that sample few-shot classification tasks with a pre-defined shape\n",
        "train_sampler = TaskSampler(\n",
        "    train_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_tasks_per_epoch\n",
        ")\n",
        "val_sampler = TaskSampler(\n",
        "    val_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_validation_tasks\n",
        ")\n",
        "\n",
        "# Finally, the DataLoader. We customize the collate_fn so that batches are delivered\n",
        "# in the shape: (support_images, support_labels, query_images, query_labels, class_ids)\n",
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_sampler=train_sampler,\n",
        "    num_workers=n_workers,\n",
        "    pin_memory=True,\n",
        "    collate_fn=train_sampler.episodic_collate_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_set,\n",
        "    batch_sampler=val_sampler,\n",
        "    num_workers=n_workers,\n",
        "    pin_memory=True,\n",
        "    collate_fn=val_sampler.episodic_collate_fn,\n",
        ")\n",
        "\n",
        "print(len(train_set), len(val_set))"
      ],
      "metadata": {
        "id": "nmWydVXk8JYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torchvision.models as models\n",
        "from easyfsl.methods import PrototypicalNetworks, FewShotClassifier\n",
        "from easyfsl.methods import SimpleShot, FewShotClassifier\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# Load a pretrained ResNet18 model\n",
        "pretrained_resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "num_classes = 512\n",
        "pretrained_resnet18.fc = nn.Linear(pretrained_resnet18.fc.in_features, num_classes)\n",
        "\n",
        "# Ensure the model is set to the correct device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pretrained_resnet18 = pretrained_resnet18.to(DEVICE)\n",
        "\n",
        "few_shot_classifier = SimpleShot(pretrained_resnet18).to(DEVICE)\n"
      ],
      "metadata": {
        "id": "wQ3g-54A8N-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import SGD, Optimizer\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "LOSS_FUNCTION = nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 30\n",
        "scheduler_milestones = [120, 160]\n",
        "scheduler_gamma = 0.1\n",
        "learning_rate = 1e-2\n",
        "tb_logs_dir = Path(\".\")\n",
        "\n",
        "train_optimizer = SGD(\n",
        "    few_shot_classifier.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4\n",
        ")\n",
        "train_scheduler = MultiStepLR(\n",
        "    train_optimizer,\n",
        "    milestones=scheduler_milestones,\n",
        "    gamma=scheduler_gamma,\n",
        ")\n",
        "\n",
        "tb_writer = SummaryWriter(log_dir=str(tb_logs_dir))"
      ],
      "metadata": {
        "id": "cOdKOzj98RkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_epoch(\n",
        "    model: FewShotClassifier, data_loader: DataLoader, optimizer: Optimizer\n",
        "):\n",
        "    all_loss = []\n",
        "    model.train()\n",
        "    with tqdm(\n",
        "        enumerate(data_loader), total=len(data_loader), desc=\"Training\"\n",
        "    ) as tqdm_train:\n",
        "        for episode_index, (\n",
        "            support_images,\n",
        "            support_labels,\n",
        "            query_images,\n",
        "            query_labels,\n",
        "            _,\n",
        "        ) in tqdm_train:\n",
        "            optimizer.zero_grad()\n",
        "            model.process_support_set(\n",
        "                support_images.to(DEVICE), support_labels.to(DEVICE)\n",
        "            )\n",
        "            classification_scores = model(query_images.to(DEVICE))\n",
        "\n",
        "            loss = LOSS_FUNCTION(classification_scores, query_labels.to(DEVICE))\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            all_loss.append(loss.item())\n",
        "\n",
        "            tqdm_train.set_postfix(loss=mean(all_loss))\n",
        "\n",
        "    return mean(all_loss)"
      ],
      "metadata": {
        "id": "ojLSBvch8U-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from easyfsl.utils import evaluate\n",
        "\n",
        "\n",
        "best_state = few_shot_classifier.state_dict()\n",
        "best_validation_accuracy = 0.0\n",
        "for epoch in range(n_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    average_loss = training_epoch(few_shot_classifier, train_loader, train_optimizer)\n",
        "    validation_accuracy = evaluate(\n",
        "        few_shot_classifier, val_loader, device=DEVICE, tqdm_prefix=\"Validation\"\n",
        "    )\n",
        "\n",
        "    if validation_accuracy > best_validation_accuracy:\n",
        "        best_validation_accuracy = validation_accuracy\n",
        "        best_state = few_shot_classifier.state_dict()\n",
        "        print(\"Ding ding ding! We found a new best model!\")\n",
        "\n",
        "    tb_writer.add_scalar(\"Train/loss\", average_loss, epoch)\n",
        "    tb_writer.add_scalar(\"Val/acc\", validation_accuracy, epoch)\n",
        "\n",
        "    # Warn the scheduler that we did an epoch\n",
        "    # so it knows when to decrease the learning rate\n",
        "    train_scheduler.step()\n",
        "\n",
        "    # if epoch % 1 == 0 : torch.save(best_state, '/content/drive/MyDrive/Thesis_code/real_5shot_5way_BanglaPrototypicalNetworks_BanglaLekha_Isolated.pth')"
      ],
      "metadata": {
        "id": "IG6pA52a8ZAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PATH = '/content/drive/MyDrive/PatternRecognition-main/ML/Proto/5shot_5way_BanglaPrototypicalNetworks_BanglaLekha_Isolated.pth'\n",
        "# few_shot_classifier.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n"
      ],
      "metadata": {
        "id": "ixoYLWAYeMfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/PatternRecognition-main/ML/Proto/5shot_5way_BanglaPrototypicalNetworks_BanglaLekha_Isolated.pth'\n",
        "few_shot_classifier.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "jD---wAY8d1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "6-YALOOBJstS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_test_tasks = 100\n",
        "\n",
        "test_set.get_labels = lambda: [\n",
        "    instance[1] for instance in test_set\n",
        "]\n",
        "\n",
        "test_sampler = TaskSampler(\n",
        "    test_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_test_tasks\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_set,\n",
        "    batch_sampler=test_sampler,\n",
        "    num_workers=n_workers,\n",
        "    pin_memory=True,\n",
        "    collate_fn=test_sampler.episodic_collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "eEakeF2Y8h-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from easyfsl.utils import evaluate\n",
        "accuracy = evaluate(few_shot_classifier, test_loader, device=DEVICE)\n",
        "print(f\"Average accuracy : {(100 * accuracy):.2f} %\")"
      ],
      "metadata": {
        "id": "zGp8gjVZ8k7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import torch\n",
        "\n",
        "\n",
        "def evaluate_with_metrics(model, loader, device):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, (support_images, support_labels, query_images, query_labels, _) in enumerate(loader):\n",
        "            model.process_support_set(support_images.to(device), support_labels.to(device))\n",
        "            outputs = model(query_images.to(device))\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_labels.extend(query_labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "    return precision, recall, f1_score\n"
      ],
      "metadata": {
        "id": "Y6EGdusfEs3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1_score = evaluate_with_metrics(few_shot_classifier, test_loader, DEVICE)\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1_score:.2f}\")"
      ],
      "metadata": {
        "id": "uE5xJeMYEuTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image_with_support_set(query_image_path):\n",
        "    # Load and transform the query image\n",
        "    query_image = Image.open(query_image_path).convert('RGB')\n",
        "    query_image_tensor = image_transforms(query_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Set the model in evaluation mode\n",
        "    few_shot_classifier.eval()\n",
        "\n",
        "    # Use the model to predict\n",
        "    with torch.no_grad():\n",
        "        few_shot_classifier.process_support_set(support_images_tensor.to(DEVICE), support_labels_tensor.to(DEVICE))\n",
        "        outputs = few_shot_classifier(query_image_tensor.to(DEVICE))\n",
        "\n",
        "    probabilities = torch.softmax(outputs, dim=1)\n",
        "    predicted_label_idx = probabilities.argmax(1).item()\n",
        "    predicted_label = support_labels[predicted_label_idx] + 1  # Adjust label to match original range\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "label_to_bengali = {\n",
        "    1: \"অ\",\n",
        "    2: \"আ\",\n",
        "    3: \"ই\",\n",
        "    4: \"ঈ\",\n",
        "    5: \"উ\",\n",
        "    6: \"ঊ\",\n",
        "    7: \"ঋ\",\n",
        "    8: \"এ\",\n",
        "    9: \"ঐ\",\n",
        "    10: \"ও\",\n",
        "    11: \"ঔ\",\n",
        "    12: \"ক\",\n",
        "    13: \"খ\",\n",
        "    14: \"গ\",\n",
        "    15: \"ঘ\",\n",
        "    16: \"ঙ\",\n",
        "    17: \"চ\",\n",
        "    18: \"ছ\",\n",
        "    19: \"জ\",\n",
        "    20: \"ঝ\",\n",
        "    21: \"ঞ\",\n",
        "    22: \"ট\",\n",
        "    23: \"ঠ\",\n",
        "    24: \"ড\",\n",
        "    25: \"ঢ\",\n",
        "    26: \"ণ\",\n",
        "    27: \"ত\",\n",
        "    28: \"থ\",\n",
        "    29: \"দ\",\n",
        "    30: \"ধ\",\n",
        "    31: \"ন\",\n",
        "    32: \"প\",\n",
        "    33: \"ফ\",\n",
        "    34: \"ব\",\n",
        "    35: \"ভ\",\n",
        "    36: \"ম\",\n",
        "    37: \"য\",\n",
        "    38: \"র\",\n",
        "    39: \"ল\",\n",
        "    40: \"শ\",\n",
        "    41: \"ষ\",\n",
        "    42: \"স\",\n",
        "    43: \"হ\",\n",
        "    44: \"ড়\",\n",
        "    45: \"ঢ়\",\n",
        "    46: \"য়\",\n",
        "    47: \"ৎ\",\n",
        "    48: \"ং\",\n",
        "    49: \"ঃ\",\n",
        "    50: \"ঁ\",\n",
        "    51: \"০\",\n",
        "    52: \"১\",\n",
        "    53: \"২\",\n",
        "    54: \"৩\",\n",
        "    55: \"৪\",\n",
        "    56: \"৫\",\n",
        "    57: \"৬\",\n",
        "    58: \"৭\",\n",
        "    59: \"৮\",\n",
        "    60: \"৯\",\n",
        "}\n",
        "\n",
        "\n",
        "# Function to print the Bengali character for a given label\n",
        "def print_bengali_character(label):\n",
        "    character = label_to_bengali.get(label, \"Unknown label\")\n",
        "    print(character)\n",
        "\n",
        "\n",
        "# Define the image preprocessing\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((84, 84)),  # Resize the image to what the model expects\n",
        "    transforms.ToTensor(),        # Convert the image to a tensor\n",
        "])\n",
        "\n",
        "# Define the root directory of your support images\n",
        "root_dir = '/content/data/BanglaLekha_Isolated_mod/Test/'\n",
        "\n",
        "# Generate support set\n",
        "support_images = []\n",
        "support_labels = []\n",
        "\n",
        "# Assuming each class directory contains images for that class\n",
        "for label in range(1, 12):  # Labels from 1 to 60\n",
        "    class_dir = os.path.join(root_dir, str(label))\n",
        "    if not os.path.exists(class_dir):\n",
        "        continue  # Skip if the class directory doesn't exist\n",
        "\n",
        "    # List all images in the class directory\n",
        "    img_files = [f for f in os.listdir(class_dir) if f.endswith('.png')]\n",
        "    for img_file in img_files[:1]:  # Take only the first image for simplicity; adjust as needed\n",
        "        img_path = os.path.join(class_dir, img_file)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        support_images.append(image_transforms(image))\n",
        "        support_labels.append(label - 1)  # Adjust label to start from 0\n",
        "\n",
        "# Convert lists to tensors\n",
        "support_images_tensor = torch.stack(support_images)\n",
        "support_labels_tensor = torch.tensor(support_labels)\n",
        "\n",
        "query_image_path = '/content/mota_dho.jpg'\n",
        "predicted_label = predict_image_with_support_set(query_image_path)\n",
        "print(\"Predicted label: \")\n",
        "print_bengali_character(predicted_label)"
      ],
      "metadata": {
        "id": "Q_KbMACHrrbj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}