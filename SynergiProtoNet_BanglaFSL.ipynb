{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWhTxhYS7TLf",
    "outputId": "adfb51c0-61b0-4dc0-9db2-12db197fb304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnxSUVIK73xa"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    colab = True\n",
    "except:\n",
    "    colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5vrGDXf7_Ly",
    "outputId": "deaf9a13-4b34-46dc-9a2d-69d98288da56"
   },
   "outputs": [],
   "source": [
    "if colab is True:\n",
    "    # Running in Google Colab\n",
    "    # Clone the repo\n",
    "    !git clone https://github.com/sicara/easy-few-shot-learning\n",
    "    %cd easy-few-shot-learning\n",
    "    !pip install .\n",
    "else:\n",
    "    # Run locally\n",
    "    # Ensure working directory is the project's root\n",
    "    # Make sure easyfsl is installed!\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NahoUBngbPi5"
   },
   "source": [
    "#Download the Dataset or Upload the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "regVTEJRagaY",
    "outputId": "ad6b4383-baef-41af-b424-c4d5b3b31811"
   },
   "outputs": [],
   "source": [
    "# Install gdown if not already installed\n",
    "!pip install gdown\n",
    "\n",
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Google Drive file ID\n",
    "file_id = \"\"\n",
    "destination = \"\"\n",
    "\n",
    "# Construct the download URL\n",
    "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(download_url, destination, quiet=False)\n",
    "\n",
    "# Unzip the file\n",
    "unzip_dir = \"/content/data\"\n",
    "os.makedirs(unzip_dir, exist_ok=True)\n",
    "with zipfile.ZipFile(destination, 'r') as zip_ref:\n",
    "    zip_ref.extractall(unzip_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Nd7psZmn8I9",
    "outputId": "52888567-3e7a-4d72-a076-50bffde4a685"
   },
   "outputs": [],
   "source": [
    "# !unzip /content/BasicFinalDatabase.zip -d /content/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhiI49Vl8DkZ"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JE-g7hat8EWR"
   },
   "outputs": [],
   "source": [
    "random_seed = 30\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0z4qdEF-9Nhq"
   },
   "source": [
    "# Episodic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoaV-tbj8HJw"
   },
   "outputs": [],
   "source": [
    "n_way = 3                   #change this for 10 way or 50 way\n",
    "n_shot = 1                  #change this for 5 shot or 10 shot\n",
    "n_query = 10\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ii6KTrFt1u5O"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class BengaliCharactersDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                if img_file.endswith('.png'):\n",
    "                    self.samples.append((os.path.join(class_dir, img_file), class_name))\n",
    "\n",
    "        # Debug: Print the first few samples\n",
    "        print(\"First few samples:\")\n",
    "        print(self.samples[:5])  # Adjust the number to print more or fewer samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_str = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Convert label from string to integer\n",
    "        label = self.class_to_idx[label_str]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [label for _, label in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0jujLP9Aqlh"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class BengaliCharactersDataset2(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                if img_file.endswith('.bmp'):\n",
    "                    self.samples.append((os.path.join(class_dir, img_file), class_name))\n",
    "\n",
    "        # Debug: Print the first few samples\n",
    "        print(\"First few samples:\")\n",
    "        print(self.samples[:5])  # Adjust the number to print more or fewer samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_str = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Convert label from string to integer\n",
    "        label = self.class_to_idx[label_str]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [label for _, label in self.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lTdyVps2RLk",
    "outputId": "0ca69f7e-7ed1-4e32-ee8e-d1729069ad3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few samples:\n",
      "[('/content/data/NumtaDB_merged/Train/3/a07976.png', '3'), ('/content/data/NumtaDB_merged/Train/3/a10586.png', '3'), ('/content/data/NumtaDB_merged/Train/3/a15641.png', '3'), ('/content/data/NumtaDB_merged/Train/3/a17436.png', '3'), ('/content/data/NumtaDB_merged/Train/3/d03506.png', '3')]\n",
      "First few samples:\n",
      "[('/content/data/NumtaDB_merged/Test/7/a17716.png', '7'), ('/content/data/NumtaDB_merged/Test/7/a15061.png', '7'), ('/content/data/NumtaDB_merged/Test/7/a11774.png', '7'), ('/content/data/NumtaDB_merged/Test/7/a09955.png', '7'), ('/content/data/NumtaDB_merged/Test/7/d06928.png', '7')]\n"
     ]
    }
   ],
   "source": [
    "image_size = 84  # Adjusted for ResNet, which typically expects larger input sizes\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),  # Random rotation between -10 and 10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_set = BengaliCharactersDataset(root_dir='/content/data/NumtaDB_merged/Train', transform=train_transforms)\n",
    "test_set = BengaliCharactersDataset(root_dir='/content/data/NumtaDB_merged/Test', transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmWydVXk8JYI",
    "outputId": "d726839c-ccd7-407b-bbc3-ec69f1f81eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18487 12123\n"
     ]
    }
   ],
   "source": [
    "from easyfsl.samplers import TaskSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "n_tasks_per_epoch = 500\n",
    "n_validation_tasks = 100\n",
    "\n",
    "val_set = test_set\n",
    "\n",
    "train_set.get_labels = lambda: [\n",
    "    instance[1] for instance in train_set\n",
    "]\n",
    "\n",
    "val_set.get_labels = lambda: [\n",
    "    instance[1] for instance in val_set\n",
    "]\n",
    "\n",
    "# Those are special batch samplers that sample few-shot classification tasks with a pre-defined shape\n",
    "train_sampler = TaskSampler(\n",
    "    train_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_tasks_per_epoch\n",
    ")\n",
    "val_sampler = TaskSampler(\n",
    "    val_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_validation_tasks\n",
    ")\n",
    "\n",
    "# Finally, the DataLoader. We customize the collate_fn so that batches are delivered\n",
    "# in the shape: (support_images, support_labels, query_images, query_labels, class_ids)\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_sampler=train_sampler,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=train_sampler.episodic_collate_fn,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_sampler=val_sampler,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=val_sampler.episodic_collate_fn,\n",
    ")\n",
    "\n",
    "print(len(train_set), len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-leA0OoxYQft",
    "outputId": "1e9efa20-4a6a-4546-cf5b-622665768178"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from easyfsl.methods import PrototypicalNetworks, FewShotClassifier\n",
    "# Define the CNN Encoder\n",
    "class CNNEncoder(nn.Module):\n",
    "    \"\"\"CNN Encoder for feature extraction.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(64, momentum=1, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(64, momentum=1, affine=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64, momentum=1, affine=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64, momentum=1, affine=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = out.view(out.size(0), -1)  # Flatten the output tensor\n",
    "        return out  # Return the 1-dimensional tensor for feature embedding\n",
    "\n",
    "# Load a pretrained ResNet18 model and modify it to output embeddings\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])  # Remove the final classification layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "# Instantiate the CNN Encoder and the ResNet Encoder\n",
    "cnn_encoder = CNNEncoder()\n",
    "pretrained_resnet18 = models.resnet18(pretrained=True)\n",
    "resnet_encoder = ResNetEncoder(pretrained_resnet18)\n",
    "\n",
    "# Combine the features from both encoders\n",
    "class CombinedEncoder(nn.Module):\n",
    "    def __init__(self, cnn_encoder, resnet_encoder):\n",
    "        super(CombinedEncoder, self).__init__()\n",
    "        self.cnn_encoder = cnn_encoder\n",
    "        self.resnet_encoder = resnet_encoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_features = self.cnn_encoder(x)\n",
    "        resnet_features = self.resnet_encoder(x)\n",
    "        combined_features = torch.cat((cnn_features, resnet_features), dim=1)\n",
    "        return combined_features\n",
    "\n",
    "# Instantiate the Combined Encoder\n",
    "combined_encoder = CombinedEncoder(cnn_encoder, resnet_encoder)\n",
    "\n",
    "# Ensure the model is set to the correct device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "combined_encoder = combined_encoder.to(DEVICE)\n",
    "\n",
    "# Create the few-shot classifier with the Combined Encoder\n",
    "few_shot_classifier = PrototypicalNetworks(combined_encoder).to(DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOdKOzj98RkB"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Optimizer\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "LOSS_FUNCTION = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 30\n",
    "scheduler_milestones = [120, 160]\n",
    "scheduler_gamma = 0.1\n",
    "learning_rate = 1e-2\n",
    "tb_logs_dir = Path(\".\")\n",
    "\n",
    "train_optimizer = SGD(\n",
    "    few_shot_classifier.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4\n",
    ")\n",
    "train_scheduler = MultiStepLR(\n",
    "    train_optimizer,\n",
    "    milestones=scheduler_milestones,\n",
    "    gamma=scheduler_gamma,\n",
    ")\n",
    "\n",
    "tb_writer = SummaryWriter(log_dir=str(tb_logs_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojLSBvch8U-a"
   },
   "outputs": [],
   "source": [
    "def training_epoch(\n",
    "    model: FewShotClassifier, data_loader: DataLoader, optimizer: Optimizer\n",
    "):\n",
    "    all_loss = []\n",
    "    model.train()\n",
    "    with tqdm(\n",
    "        enumerate(data_loader), total=len(data_loader), desc=\"Training\"\n",
    "    ) as tqdm_train:\n",
    "        for episode_index, (\n",
    "            support_images,\n",
    "            support_labels,\n",
    "            query_images,\n",
    "            query_labels,\n",
    "            _,\n",
    "        ) in tqdm_train:\n",
    "            optimizer.zero_grad()\n",
    "            model.process_support_set(\n",
    "                support_images.to(DEVICE), support_labels.to(DEVICE)\n",
    "            )\n",
    "            classification_scores = model(query_images.to(DEVICE))\n",
    "\n",
    "            loss = LOSS_FUNCTION(classification_scores, query_labels.to(DEVICE))\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            all_loss.append(loss.item())\n",
    "\n",
    "            tqdm_train.set_postfix(loss=mean(all_loss))\n",
    "\n",
    "    return mean(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IG6pA52a8ZAZ",
    "outputId": "09764fa7-4dd2-4535-d5be-ae4d58cdfb84"
   },
   "outputs": [],
   "source": [
    "from easyfsl.utils import evaluate\n",
    "\n",
    "\n",
    "best_state = few_shot_classifier.state_dict()\n",
    "best_validation_accuracy = 0.0\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    average_loss = training_epoch(few_shot_classifier, train_loader, train_optimizer)\n",
    "    validation_accuracy = evaluate(\n",
    "        few_shot_classifier, val_loader, device=DEVICE, tqdm_prefix=\"Validation\"\n",
    "    )\n",
    "\n",
    "    if validation_accuracy > best_validation_accuracy:\n",
    "        best_validation_accuracy = validation_accuracy\n",
    "        best_state = few_shot_classifier.state_dict()\n",
    "        print(\"Ding ding ding! We found a new best model!\")\n",
    "\n",
    "    tb_writer.add_scalar(\"Train/loss\", average_loss, epoch)\n",
    "    tb_writer.add_scalar(\"Val/acc\", validation_accuracy, epoch)\n",
    "\n",
    "    # Warn the scheduler that we did an epoch\n",
    "    # so it knows when to decrease the learning rate\n",
    "    train_scheduler.step()\n",
    "\n",
    "    if epoch % 1 == 0 : torch.save(best_state, '/content/1shot_5way_hindi_BFD.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixoYLWAYeMfH"
   },
   "outputs": [],
   "source": [
    "# PATH = '/content/drive/MyDrive/PatternRecognition-main/ML/Proto/5shot_5way_BanglaPrototypicalNetworks_BanglaLekha_Isolated.pth'\n",
    "# few_shot_classifier.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jD---wAY8d1J"
   },
   "outputs": [],
   "source": [
    "PATH = '/content/1shot_5way_hindi_BFD.pth'\n",
    "few_shot_classifier.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckqfZP6dbGX5"
   },
   "source": [
    "#Code for merging Train and Test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxB-DV4TKUnW"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# train_dir = '/content/data/training-c-processed/Train'\n",
    "# test_dir = '/content/data/training-c-processed/Test'\n",
    "# new_test_dir = '/content/data/training-c-processed/New_Test'\n",
    "\n",
    "# os.makedirs(new_test_dir, exist_ok=True)\n",
    "\n",
    "# def copy_files(src_dir, dest_dir):\n",
    "#     for class_name in os.listdir(src_dir):\n",
    "#         class_path = os.path.join(src_dir, class_name)\n",
    "#         if os.path.isdir(class_path):\n",
    "#             dest_class_path = os.path.join(dest_dir, class_name)\n",
    "#             os.makedirs(dest_class_path, exist_ok=True)\n",
    "#             for img_file in os.listdir(class_path):\n",
    "#                 src_img_path = os.path.join(class_path, img_file)\n",
    "#                 dest_img_path = os.path.join(dest_class_path, img_file)\n",
    "#                 shutil.copy(src_img_path, dest_img_path)\n",
    "\n",
    "# copy_files(train_dir, new_test_dir)\n",
    "# copy_files(test_dir, new_test_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-YALOOBJstS"
   },
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEakeF2Y8h-p"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_test_tasks = 100\n",
    "test_set.get_labels = lambda: [\n",
    "    instance[1] for instance in test_set\n",
    "]\n",
    "\n",
    "test_sampler = TaskSampler(\n",
    "    test_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_test_tasks\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_sampler=test_sampler,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=test_sampler.episodic_collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGp8gjVZ8k7p"
   },
   "outputs": [],
   "source": [
    "from easyfsl.utils import evaluate\n",
    "accuracy = evaluate(few_shot_classifier, test_loader, device=DEVICE)\n",
    "print(f\"Average accuracy : {(100 * accuracy):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6EGdusfEs3A"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch\n",
    "\n",
    "\n",
    "def evaluate_with_metrics(model, loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (support_images, support_labels, query_images, query_labels, _) in enumerate(loader):\n",
    "            model.process_support_set(support_images.to(device), support_labels.to(device))\n",
    "            outputs = model(query_images.to(device))\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(query_labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "    return precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uE5xJeMYEuTq"
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score = evaluate_with_metrics(few_shot_classifier, test_loader, DEVICE)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
